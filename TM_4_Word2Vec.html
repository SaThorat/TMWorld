<!DOCTYPE html>
<html>
<head>
    <title>Introduction to Word2vec </title>
	<link rel="stylesheet" href="CSS/style.css">
</head>
<body>
	<h1>Introduction to Word2vec </h1>
    <p>Word2vec builds word vectors which are representations of words. The vectors we use to represent words are called neural word embedding’s. Word2vec was developed by researchers Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean at Google in 2013. Google hosts an open-source version of Word2vec released under an Apache 2.0 license.</p>
    <p>The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors viz. feature vectors that represent words in that corpus.</p>
    <p>The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors. The purpose and usefulness of Word2vec is to group the vectors of similar words together in vector space. Word2vec creates vectors that are distributed numerical representations of word features such as the context of individual words. </p>
    <strong> Key Points Related with Word2vec  :</strong>
    <div style="font-family: cursive;">
    <ol>
        <li>Word2vec  represents words as dense vectors in a continuous vector space. This representation allows words with similar meanings or context to have similar vector representations, which is valuable for various NLP tasks.</li>
        <li>Capturing semantic relationships: Word2vec  is known for capturing semantic relationships between words. For instance, words with similar meanings, such as "king" and "queen," will have similar vector representations, and the vector arithmetic "king - man + woman" is likely to yield a vector representation close to "queen."</li>
        <li>Training Word2vec is computationally efficient, especially compared to older methods for word representation. It can be trained on large amounts of text data and still produce meaningful word embedding’s.</li>
        <li>Word2vec allows for pre-training on large text corpora, and the pre-trained models can be used for various downstream tasks, even on smaller datasets with specific domain vocabularies.</li>
        <li>Word2vec embedding’s have been shown to improve the performance of various NLP tasks, such as sentiment analysis, named entity recognition, machine translation, and text classification.</li>
</ol>
</div>


    <strong>Two variants of Word2vec :</strong>
    <p>Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuous bag-of-words (CBOW) or continuous skip-gram. In both architectures, word2vec considers both individual words and a sliding window of context words surrounding individual words as it iterates over the entire corpus. Both CBOW and Skip-gram methods use a hidden layer in the neural network to learn the word embeddings. They use the softmax function to convert the output layer into probability scores, which are then used to train the model through backpropagation. These probability scores represent the likelihood of each word in the vocabulary being a context word or a target word given the input context or target word. Negative sampling is a subsampling method used to minimize word similarity when it occurs in different contexts and maximize word similarity when it occurs in the same context.</p>
    
    <h4 style="color: blue;">1.	Continuous Bag of Words (CBOW): </h4>
    <p>In CBOW, the model aims to predict a target word from its surrounding context words (known as the context window). This architecture is effective for smaller datasets and tends to capture global word relationships. The order of context words does not influence prediction (bag-of-words assumption). </p>
    <p> Input: Context words (word embedding’s of surrounding words)<br>
     Output: Target word (word embedding of the central word)
    </p>
    <p>The CBOW method is faster to train and performs well when the context window is small, making it suitable for capturing global word relationships in the text.</p>
    <p>Example : "It is a pleasant day."<br>
        Input :  it is a pleasant<br>
        Word pairing of window size 2 : ([it, a], is), ([is, nice], a) ([a, day], pleasant).<br>
        Output : day
    </p>
    
    <h4 style="color: blue;">2.	Skip-gram model : </h4>
    <p>The skip-gram model is the inverse of CBOW. It aims to predict the context words given a target word The skip-gram architecture weighs nearby context words more heavily than more distant context words.. Skip-gram is more effective for larger datasets and can capture local word relationships more effectively. </p>
    <p> Input: Target word (word embedding of the central word)<br>
        Output: Context words (word embedding’s of surrounding words)<br>
    </p>
    <p>
    Example <br>
       Sentence : ['Best way to success is through hard work and persistence']<br>
        1.	Target word : Best<br>
       Context word : way<br>
       2.	Target word : way<br>
       Context word : Best , to
       </p>

    <h4><strong>Link for the Git Code :</strong></h4>
    <a href="https://github.com/SaThorat/TMCodes/blob/main/Word2Vec%20using%20gensim.ipynb" target="_blank">https://github.com/SaThorat/TMCodes/blob/main/BagofWords.ipynb</a>
    
    <h3>References:</h3>
    <ul>
        <li><a href="https://towardsdatascience.com/word2vec-explained-49c52b4ccb71" target="_blank">https://towardsdatascience.com/word2vec-explained-49c52b4ccb71</a></li>
        <li><a href="https://www.tensorflow.org/text/tutorials/word2vec" target="_blank">https://www.tensorflow.org/text/tutorials/word2vec</a></li>
        <li><a href="https://www.scaler.com/topics/nlp/word2vec/" target="_blank">https://www.scaler.com/topics/nlp/word2vec/</a></li>
        <li><a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank">https://en.wikipedia.org/wiki/Word2vec</a></li>
        <li><a href="https://wiki.pathmind.com/word2vec#sequence" target="_blank">https://wiki.pathmind.com/word2vec#sequence</a></li>

    </ul>
	<h3>Contributed by:</h3>
	<ul>
		<li><a href="https://github.com/SaThorat" target="_blank">Dr. Sandeep Thorat</a></li>
	    <li><a href="https://github.com/Aishwarya-Kotkar" target="_blank">Ms. Aishwarya Kotkar</a></li>
	</ul>
	<p style="text-align: end; color: black; font-size: large;"><a href="textMining.html" target="middleFrame">Go back</a></p>
</body>
</html>
